{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from scipy import integrate\n",
    "from scipy.optimize import fmin_powell, fminbound, brentq\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Synthesize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to create synthetic data\n",
    "def irt_evaluation(difficulty, discrimination, thetas):\n",
    "    \"\"\"\n",
    "        Evaluates an IRT model and returns the exact values\n",
    "        \n",
    "        Args:\n",
    "            difficulty: [array] of difficulty parameters\n",
    "            discrimination:  [array | number] of discrimination parameters\n",
    "            thetas: [array] of person abilities\n",
    "            \n",
    "        Returns:\n",
    "            dichotomous matrix of [difficulty.size x thetas.size] representing\n",
    "            synthetic data\n",
    "    \"\"\"\n",
    "    # If discrimination is a scalar, make it an array\n",
    "    if not np.ndim(discrimination):\n",
    "        discrimination = np.ones_like(difficulty) * discrimination\n",
    "\n",
    "    kernel = difficulty[:, None] - thetas\n",
    "    kernel *= discrimination[:, None]\n",
    "    return 1.0 / (1 + np.exp(kernel))\n",
    "\n",
    "\n",
    "\n",
    "def create_synthetic_irt_dichotomous(difficulty, discrimination, thetas):\n",
    "    \"\"\"\n",
    "        Creates synthetic IRT data to test parameters estimation\n",
    "        functions.  Only for use with dichotomous outputs\n",
    "        \n",
    "        Args:\n",
    "            difficulty: [array] of difficulty parameters\n",
    "            discrimination:  [array | number] of discrimination parameters\n",
    "            thetas: [array] of person abilities\n",
    "            \n",
    "        Returns:\n",
    "            dichotomous matrix of [difficulty.size x thetas.size] representing\n",
    "            synthetic data\n",
    "    \"\"\"\n",
    "    continuous_output = irt_evaluation(difficulty, discrimination, thetas)\n",
    "\n",
    "    # convert to binary based on probability\n",
    "    random_compare = np.random.rand(*continuous_output.shape)\n",
    "    \n",
    "    return random_compare <= continuous_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import roots_legendre\n",
    "\n",
    "\n",
    "def _get_quadrature_points(n, a, b):\n",
    "    \"\"\"\n",
    "        Utility function to get the legendre points, \n",
    "        shifted from [-1, 1] to [a, b]\n",
    "        \n",
    "        Args:\n",
    "            n: number of quadrature_points\n",
    "            a: lower bound of integration\n",
    "            b: upper bound of integration \n",
    "            \n",
    "        A local function of the based fixed_quad found in scipy\n",
    "    \"\"\"\n",
    "    x, w = roots_legendre(n)\n",
    "    x = np.real(x)\n",
    "    \n",
    "    return (b - a) * (x + 1) * 0.5 + a\n",
    "    \n",
    "\n",
    "def _compute_partial_integral(theta, difficulty, discrimination, the_sign):\n",
    "    \"\"\"\n",
    "        To be added\n",
    "    \"\"\"\n",
    "    if np.ndim(discrimination) < 1:\n",
    "        discrimination = np.full(the_sign.shape[0], discrimination)\n",
    "        \n",
    "    kernel = the_sign[:, :, None] * np.ones((1, 1, theta.size))\n",
    "    kernel *= discrimination[:, None, None]   \n",
    "    kernel *= (theta[None, None, :] - difficulty[:, None, None])\n",
    "    \n",
    "    # Distribution\n",
    "    gauss = 1.0 / np.sqrt(2 * np.pi) * np.exp(-np.square(theta) / 2)\n",
    "\n",
    "    return  gauss[None, :] * (1.0 / (1.0 + np.exp(kernel))).prod(axis=0).squeeze()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability Rauch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probability(dataset, discrimination=1, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            discrimination: scalar of discrimination used in model (default to 1)\n",
    "            max_iter: maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "\n",
    "    # First run mml to get coarse guess,\n",
    "    # use the mean to set identify the solution\n",
    "    betas = rauch_estimate(dataset, discrimination) * 0\n",
    "    identifying_mean = betas.mean()\n",
    "    betas -= identifying_mean\n",
    "\n",
    "    # Remove the zero and full count values\n",
    "    if(unique_sets[:, 0].sum() == 0):\n",
    "        unique_sets = np.delete(unique_sets, 0, axis=1)\n",
    "        counts = np.delete(counts, 0)\n",
    "\n",
    "    if(unique_sets[:, -1].sum() == n_items):\n",
    "        unique_sets = np.delete(unique_sets, -1, axis=1)\n",
    "        counts = np.delete(counts, -1)\n",
    "\n",
    "    response_set_sums = unique_sets.sum(axis=0)\n",
    "\n",
    "    def _denominator(betas):\n",
    "        \"\"\"Computes the symmetric functions based on the betas\n",
    "        \n",
    "         Indexes by score, left to right\n",
    "        \n",
    "        \"\"\"\n",
    "        polynomials = np.c_[np.ones_like(betas), np.exp(-betas)]\n",
    "        otpt = 1\n",
    "        \n",
    "        for polynomial in polynomials:\n",
    "            otpt = np.convolve(otpt, polynomial)\n",
    "        return otpt\n",
    " \n",
    "    for iteration in range(max_iter):\n",
    "        previous_betas = betas.copy()\n",
    "        \n",
    "        for ndx in range(n_items):\n",
    "            partial_conv = _denominator(np.delete(betas, ndx))\n",
    "            \n",
    "            def min_func(estimate):\n",
    "                betas[ndx] = estimate\n",
    "                full_convolution = np.convolve([1, np.exp(-estimate)], partial_conv)\n",
    "                \n",
    "                numerator = np.exp(-np.sum(unique_sets * betas[:,None], axis=0))\n",
    "                denominator = full_convolution[response_set_sums]\n",
    "                \n",
    "                return -np.log(numerator / denominator).dot(counts)\n",
    "            \n",
    "            betas[ndx] = fminbound(min_func, -5, 5)\n",
    "\n",
    "            # recenter\n",
    "            betas += (identifying_mean - betas.mean())\n",
    "        \n",
    "        if np.abs(betas - previous_betas).max() < 1e-3:\n",
    "            print(f'Ended in {iteration} iterations')\n",
    "            break\n",
    "            \n",
    "    return betas / discrimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate functions based on approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rauch_estimate(dataset, discrimination=1):\n",
    "    \"\"\"\n",
    "        Estimates the difficulty parameters via the approximation\n",
    "    \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            discrimination: scalar of discrimination used in model (default to 1)\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination estimates\n",
    "    \"\"\"\n",
    "    n_no = np.count_nonzero(~dataset, axis=1)\n",
    "    n_yes = np.count_nonzero(dataset, axis=1)\n",
    "    return (np.sqrt(1 + discrimination**2 / 3) * \n",
    "            np.log(n_no / n_yes) / discrimination)\n",
    "\n",
    "\n",
    "def onepl_estimate(dataset):\n",
    "    \"\"\"\n",
    "        Estimates the difficulty parameters via the approximation\n",
    "    \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    n_no = np.count_nonzero(~dataset, axis=1)\n",
    "    n_yes = np.count_nonzero(dataset, axis=1)\n",
    "    scalar = np.log(n_no / n_yes)\n",
    "\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "\n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, difficulty, discrimination, response):\n",
    "        gauss = 1.0 / np.sqrt(2 * np.pi) * np.exp(-np.square(theta) / 2)\n",
    "        kernel = the_sign[:, :, None] * np.ones((1, 1, theta.size))\n",
    "        kernel *= discrimination   \n",
    "        kernel *= (theta[None, None, :] - difficulty[:, None, None])\n",
    "        \n",
    "        return  gauss[None, :] * (1.0 / (1.0 + np.exp(kernel))).prod(axis=0).squeeze()\n",
    "\n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(estimate):\n",
    "        difficulty = np.sqrt(1 + estimate**2 / 3) * scalar / estimate\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                                    (difficulty, estimate, unique_sets), n=61)[0]\n",
    "        return -np.log(otpt).dot(counts)\n",
    "       \n",
    "    # Perform the minimization\n",
    "    discrimination = fminbound(min_func, 0.25, 10)\n",
    "    \n",
    "    return discrimination, np.sqrt(1 + discrimination**2 / 3) * scalar / discrimination\n",
    "\n",
    "\n",
    "def twopl_estimate(dataset, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates the difficulty parameters via the approximation\n",
    "    \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            max_iter:  maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "    \n",
    "    theta = _get_quadrature_points(61, -5, 5)\n",
    "\n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, discrimination, old_discrimination, \n",
    "                            difficulty, old_difficulty,\n",
    "                            partial_int, the_sign):\n",
    "        kernel1 = the_sign[:, None] * (theta[None, :] - difficulty)\n",
    "        kernel1 *= discrimination\n",
    "\n",
    "        kernel2 = the_sign[:, None] * (theta[None, :] - old_difficulty)\n",
    "        kernel2 *= old_discrimination\n",
    "\n",
    "        return partial_int * (1 + np.exp(kernel2)) / (1 + np.exp(kernel1))\n",
    "    \n",
    "    \n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(estimate, dataset, old_estimate, old_difficulty,\n",
    "                 partial_int, the_sign):\n",
    "        new_difficulty = rauch_estimate(dataset, estimate)\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                                    (estimate, old_estimate, \n",
    "                                     new_difficulty, old_difficulty,\n",
    "                                     partial_int, the_sign), n=61)[0]\n",
    "        return -np.log(otpt).dot(counts)\n",
    "       \n",
    "    # Perform the minimization\n",
    "    initial_guess = np.ones((dataset.shape[0],))\n",
    "    difficulties = rauch_estimate(dataset)\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        previous_guess = initial_guess.copy()\n",
    "        previous_difficulty = difficulties.copy()\n",
    "\n",
    "        #Quadrature evaluation for values that do not change\n",
    "        partial_int = _compute_partial_integral(theta, difficulties,\n",
    "                          initial_guess, the_sign)\n",
    "        \n",
    "        for ndx in range(n_items):\n",
    "            def min_func_local(estimate):\n",
    "                return min_func(estimate, dataset[ndx].reshape(1, -1),  \n",
    "                                previous_guess[ndx], \n",
    "                                previous_difficulty[ndx],\n",
    "                                partial_int, the_sign[ndx])\n",
    "\n",
    "            initial_guess[ndx] = fminbound(min_func_local, 0.25, 6, xtol=1e-3)\n",
    "            difficulties[ndx] = rauch_estimate(dataset[ndx].reshape(1, -1), \n",
    "                                               initial_guess[ndx])\n",
    "            \n",
    "            partial_int = quadrature_function(theta, initial_guess[ndx], \n",
    "                                              previous_guess[ndx], difficulties[ndx],\n",
    "                                              previous_difficulty[ndx],\n",
    "                                              partial_int, the_sign[ndx])            \n",
    "\n",
    "        if np.abs(initial_guess - previous_guess).max() < 1e-3:\n",
    "            break\n",
    "            \n",
    "    return initial_guess, difficulties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions based on full integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rauch_estimate_int(dataset, discrimination=1, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            discrimination: scalar of discrimination used in model (default to 1)\n",
    "            max_iter: maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    n_no = np.count_nonzero(~dataset, axis=1)\n",
    "    n_yes = np.count_nonzero(dataset, axis=1)\n",
    "    scalar = n_yes / (n_yes + n_no)\n",
    "    \n",
    "    if np.ndim(discrimination) < 1:\n",
    "        discrimination = np.full(n_items, discrimination)\n",
    "   \n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, difficulty, discrimination):\n",
    "        gauss = 1.0 / np.sqrt(2 * np.pi) * np.exp(-np.square(theta) / 2)\n",
    "        return irt_evaluation(np.array([difficulty]), np.array([discrimination]), theta) * gauss\n",
    "\n",
    "    the_parameters = np.zeros((n_items,))\n",
    "\n",
    "    # Perform the minimization\n",
    "    for ndx in range(n_items):\n",
    "        \n",
    "        # Minimize each item separately\n",
    "        def min_zero_local(estimate):\n",
    "            return (scalar[ndx] - \n",
    "                    integrate.fixed_quad(quadrature_function, -10, 10, \n",
    "                    (estimate, discrimination[ndx]), n=101)[0])\n",
    "        \n",
    "        the_parameters[ndx] = brentq(min_zero_local, -6, 6)\n",
    "            \n",
    "    return the_parameters\n",
    "\n",
    "\n",
    "def onepl_estimate_int(dataset):\n",
    "    \"\"\"\n",
    "        Estimates the difficulty parameters via the approximation\n",
    "    \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "\n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, difficulty, discrimination, response):\n",
    "        gauss = 1.0 / np.sqrt(2 * np.pi) * np.exp(-np.square(theta) / 2)\n",
    "        kernel = the_sign[:, :, None] * np.ones((1, 1, theta.size))\n",
    "        kernel *= discrimination   \n",
    "        kernel *= (theta[None, None, :] - difficulty[:, None, None])\n",
    "        \n",
    "        return  gauss[None, :] * (1.0 / (1.0 + np.exp(kernel))).prod(axis=0).squeeze()\n",
    "\n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(estimate):\n",
    "        difficulty = rauch_estimate_int(dataset, estimate)\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                                    (difficulty, estimate, unique_sets), n=61)[0]\n",
    "        return -np.log(otpt).dot(counts)\n",
    "       \n",
    "    # Perform the minimization\n",
    "    discrimination = fminbound(min_func, 0.25, 10)\n",
    "    \n",
    "    return discrimination, rauch_estimate_int(dataset, discrimination)\n",
    "\n",
    "\n",
    "def twopl_estimate_int(dataset, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates the difficulty parameters via the approximation\n",
    "    \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            max_iter:  maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "    \n",
    "    theta = _get_quadrature_points(61, -5, 5)\n",
    "\n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, discrimination, old_discrimination, \n",
    "                            difficulty, old_difficulty,\n",
    "                            partial_int, the_sign):\n",
    "        kernel1 = the_sign[:, None] * (theta[None, :] - difficulty)\n",
    "        kernel1 *= discrimination\n",
    "\n",
    "        kernel2 = the_sign[:, None] * (theta[None, :] - old_difficulty)\n",
    "        kernel2 *= old_discrimination\n",
    "\n",
    "        return partial_int * (1 + np.exp(kernel2)) / (1 + np.exp(kernel1))\n",
    "    \n",
    "    \n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(estimate, dataset, old_estimate, old_difficulty,\n",
    "                 partial_int, the_sign):\n",
    "        new_difficulty = rauch_estimate_int(dataset, estimate)\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                                    (estimate, old_estimate, \n",
    "                                     new_difficulty, old_difficulty,\n",
    "                                     partial_int, the_sign), n=61)[0]\n",
    "        return -np.log(otpt).dot(counts)\n",
    "       \n",
    "    # Perform the minimization\n",
    "    initial_guess = np.ones((dataset.shape[0],))\n",
    "    difficulties = rauch_estimate(dataset)\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        previous_guess = initial_guess.copy()\n",
    "        previous_difficulty = difficulties.copy()\n",
    "\n",
    "        #Quadrature evaluation for values that do not change\n",
    "        partial_int = _compute_partial_integral(theta, difficulties,\n",
    "                          initial_guess, the_sign)\n",
    "        \n",
    "        for ndx in range(n_items):\n",
    "            def min_func_local(estimate):\n",
    "                return min_func(estimate, dataset[ndx].reshape(1, -1),  \n",
    "                                previous_guess[ndx], \n",
    "                                previous_difficulty[ndx],\n",
    "                                partial_int, the_sign[ndx])\n",
    "\n",
    "            initial_guess[ndx] = fminbound(min_func_local, 0.25, 6, xtol=1e-3)\n",
    "            difficulties[ndx] = rauch_estimate_int(dataset[ndx].reshape(1, -1), \n",
    "                                                   initial_guess[ndx])\n",
    "            \n",
    "            partial_int = quadrature_function(theta, initial_guess[ndx], \n",
    "                                              previous_guess[ndx], difficulties[ndx],\n",
    "                                              previous_difficulty[ndx],\n",
    "                                              partial_int, the_sign[ndx])            \n",
    "\n",
    "        if np.abs(initial_guess - previous_guess).max() < 1e-3:\n",
    "            break\n",
    "            \n",
    "    return initial_guess, difficulties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions based on complete joint probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rauch_estimate_full_abstract(dataset, discrimination=1, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            discrimination: scalar of discrimination used in model (default to 1)\n",
    "            max_iter: maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "\n",
    "    theta = _get_quadrature_points(61, -5, 5)\n",
    "    \n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, difficulty, old_difficulty, partial_int, the_sign):\n",
    "        kernel1 = the_sign[:, None] * (theta[None, :] - difficulty)\n",
    "        kernel1 *= discrimination\n",
    "\n",
    "        kernel2 = the_sign[:, None] * (theta[None, :] - old_difficulty)\n",
    "        kernel2 *= discrimination\n",
    "\n",
    "        return partial_int * (1 + np.exp(kernel2)) / (1 + np.exp(kernel1))\n",
    "    \n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(difficulty, old_difficulty, partial_int, the_sign):\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                (difficulty, old_difficulty, partial_int, the_sign), n=61)[0] + 1e-23\n",
    "        return -np.log(otpt).dot(counts)\n",
    "\n",
    "    # Get approximate guess to begin with\n",
    "    initial_guess = rauch_estimate(dataset, discrimination=discrimination)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        previous_guess = initial_guess.copy()\n",
    "\n",
    "        #Quadrature evaluation for values that do not change\n",
    "        partial_int = _compute_partial_integral(theta, initial_guess,\n",
    "                          discrimination, the_sign)\n",
    "                \n",
    "        for ndx in range(n_items):\n",
    "            # Minimize each one separately\n",
    "            value = initial_guess[ndx] * 1.0\n",
    "            \n",
    "            def min_func_local(estimate):\n",
    "                return min_func(estimate, previous_guess[ndx], \n",
    "                                partial_int, the_sign[ndx])\n",
    "            \n",
    "            initial_guess[ndx] = fminbound(min_func_local, \n",
    "                                           value-0.75,\n",
    "                                           value+0.75)\n",
    "            \n",
    "            partial_int = quadrature_function(theta, initial_guess[ndx], \n",
    "                                              previous_guess[ndx], partial_int, the_sign[ndx])\n",
    "\n",
    "        if(np.abs(initial_guess - previous_guess).max() < 0.001):\n",
    "            break\n",
    "            \n",
    "    # Get the value of the cost function\n",
    "    cost = integrate.fixed_quad(lambda x: partial_int, -5, 5, n=61)[0]\n",
    "    \n",
    "    return initial_guess, -np.log(cost).dot(counts)\n",
    "\n",
    "\n",
    "def rauch_estimate_full(dataset, discrimination=1, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            discrimination: scalar of discrimination used in model (default to 1)\n",
    "            max_iter: maximum number of iterations to run\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination estimates\n",
    "    \"\"\"\n",
    "    return _rauch_estimate_full_abstract(dataset, discrimination, max_iter)[0]\n",
    "\n",
    "\n",
    "def onepl_estimate_full(dataset, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    def min_func_local(estimate):\n",
    "        _, cost = _rauch_estimate_full_abstract(dataset, estimate, max_iter)\n",
    "        return cost\n",
    "    \n",
    "    discrimination = fminbound(min_func_local, 0.5, 4)\n",
    "    \n",
    "    return discrimination, rauch_estimate_full(dataset, discrimination)\n",
    "\n",
    "\n",
    "def twopl_estimate_full(dataset, max_iter=25):\n",
    "    \"\"\"\n",
    "        Estimates parameters in an IRT model with full        \n",
    "        gaussian quadrature\n",
    "        \n",
    "        Args:\n",
    "            dataset: [items x participants] matrix of True/False Values\n",
    "            \n",
    "        Returns:\n",
    "            array of discrimination, difficulty estimates\n",
    "    \"\"\"\n",
    "    n_items = dataset.shape[0]\n",
    "    unique_sets, counts = np.unique(dataset, axis=1, return_counts=True)\n",
    "    the_sign = (-1)**unique_sets\n",
    "\n",
    "    theta = _get_quadrature_points(61, -5, 5)\n",
    "    \n",
    "    # Inline definition of quadrature function\n",
    "    def quadrature_function(theta, estimates, old_estimates, partial_int, the_sign):\n",
    "        kernel1 = the_sign[:, None] * (theta[None, :] - estimates[1])\n",
    "        kernel1 *= estimates[0]\n",
    "\n",
    "        kernel2 = the_sign[:, None] * (theta[None, :] - old_estimates[1])\n",
    "        kernel2 *= old_estimates[0]\n",
    "\n",
    "        return partial_int * (1 + np.exp(kernel2)) / (1 + np.exp(kernel1))\n",
    "    \n",
    "    # Inline definition of cost function to minimize\n",
    "    def min_func(estimates, old_estimates, partial_int, the_sign):\n",
    "        otpt = integrate.fixed_quad(quadrature_function, -5, 5, \n",
    "                (estimates, old_estimates, partial_int, the_sign), n=61)[0] + 1e-23\n",
    "        return -np.log(otpt).dot(counts)\n",
    "\n",
    "    # Get approximate guess to begin with rasch model\n",
    "    a1, b1 = twopl_estimate(dataset)\n",
    "    initial_guess = np.c_[a1, b1]\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        previous_guess = initial_guess.copy()\n",
    "\n",
    "        #Quadrature evaluation for values that do not change\n",
    "        partial_int = _compute_partial_integral(theta, initial_guess[:, 1],\n",
    "                          initial_guess[:, 0], the_sign)\n",
    "                \n",
    "        for ndx in range(n_items):\n",
    "            # Minimize each one separately\n",
    "            value = initial_guess[ndx] * 1.0\n",
    "            \n",
    "            def min_func_local(estimate):\n",
    "                return min_func(estimate, previous_guess[ndx], \n",
    "                                partial_int, the_sign[ndx])\n",
    "\n",
    "            initial_guess[ndx] = fmin_powell(min_func_local, value, xtol=1e-3, disp=0)\n",
    "            partial_int = quadrature_function(theta, initial_guess[ndx], \n",
    "                                              previous_guess[ndx], partial_int, the_sign[ndx])\n",
    "\n",
    "        if(np.abs(initial_guess - previous_guess).max() < 0.001):\n",
    "            break\n",
    "                \n",
    "    return initial_guess[:, 0], initial_guess[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a set of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items, n_participants = 100, 2000\n",
    "diffc = np.linspace(-2.5, 2.5, n_items)\n",
    "discr = 1.26#1.0 + np.random.rand(n_items,)\n",
    "thetas = np.random.randn(n_participants)\n",
    "\n",
    "syn_data = create_synthetic_irt_dichotomous(diffc, discr, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_est = rauch_estimate(syn_data, 1.0)\n",
    "b_int = rauch_estimate_int(syn_data, 1.0)\n",
    "b_full = rauch_estimate_full(syn_data, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_est = onepl_estimate(syn_data)\n",
    "a_int = onepl_estimate_int(syn_data)\n",
    "a_full = onepl_estimate_full(syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_est = twopl_estimate(syn_data)\n",
    "c_int = twopl_estimate_int(syn_data)\n",
    "c_full = twopl_estimate_full(syn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14253848245771353, 0.13211132771089557)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(c_est[0] - discr).mean()), np.sqrt(np.square(c_est[1] - diffc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1326282901494772, 0.11324222526558021)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(c_int[0] - discr).mean()), np.sqrt(np.square(c_int[1] - diffc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13417876733982936, 0.11486588565924734)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(c_full[0] - discr).mean()), np.sqrt(np.square(c_full[1] - diffc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex(4, 3).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended in 5 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.44665315, -2.37710512, -2.37710663, -2.21659607, -2.33491451,\n",
       "       -2.32900824, -2.17957588, -2.0842642 , -2.09884573, -2.01828887,\n",
       "       -1.94670522, -1.88701747, -1.97310095, -1.89958786, -1.79806248,\n",
       "       -1.7670296 , -1.84183657, -1.68125726, -1.62780363, -1.53921145,\n",
       "       -1.48055269, -1.35390771, -1.41764327, -1.30677368, -1.28363769,\n",
       "       -1.19097865, -1.21860568, -1.19097245, -1.09176345, -1.0734858 ,\n",
       "       -1.04249018, -1.02969454, -0.89077256, -0.8381402 , -0.76094545,\n",
       "       -0.72877062, -0.61408507, -0.6563777 , -0.60082395, -0.51135818,\n",
       "       -0.47264454, -0.4003204 , -0.39398402, -0.33934767, -0.25819571,\n",
       "       -0.24578317, -0.18190266, -0.12040419, -0.08974328, -0.1020026 ,\n",
       "        0.05910308,  0.02037843,  0.04279489,  0.21862969,  0.17348551,\n",
       "        0.27634004,  0.30325418,  0.33025655,  0.47543265,  0.48609629,\n",
       "        0.56575638,  0.61821115,  0.70274285,  0.65134755,  0.74349728,\n",
       "        0.82659039,  0.81491317,  0.95587033,  0.9145028 ,  0.9632377 ,\n",
       "        1.07651276,  1.09203626,  1.15524353,  1.19027076,  1.32222504,\n",
       "        1.28482782,  1.3251315 ,  1.41762172,  1.38726669,  1.59493716,\n",
       "        1.52173001,  1.58814344,  1.68245313,  1.6932988 ,  1.81421242,\n",
       "        1.76387756,  1.76768764,  1.89950756,  2.09790544,  1.91213349,\n",
       "        2.01308424,  2.00399654,  2.14253701,  2.21532107,  2.17320435,\n",
       "        2.33344878,  2.33937936,  2.43219362,  2.39416393,  2.5332861 ])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_probability(syn_data, 1.26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00016840622602676714"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv = rauch_estimate(syn_data, 1.26)\n",
    "vv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _denominator(betas):\n",
    "        \"\"\"Computes the symmetric functions based on the betas\n",
    "        \n",
    "         Indexes by score, left to right\n",
    "        \n",
    "        \"\"\"\n",
    "        new_length = betas.shape[0] + 1\n",
    "        polynomials = np.c_[np.ones_like(betas), np.exp(-betas)]\n",
    "        spectrum = np.fft.fft(polynomials, n=new_length, norm=None, axis=1)\n",
    "        otpt = 1\n",
    "        \n",
    "        for s in spectrum:\n",
    "            otpt *= s\n",
    "            \n",
    "        return  np.fft.ifft(otpt).real\n",
    "        \n",
    "#         return np.fft.ifft(np.prod(scale * np.fft.fft(polynomials, n=new_length, norm=None, axis=1), \n",
    "#                                    axis=0), norm=None).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomials = np.c_[np.ones_like(diffc), np.exp(-diffc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.      , 0.082085])"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomials[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = 60\n",
    "diffc = np.linspace(-2.5, 2.5, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.50074555e+07, -3.06441208e+07, -7.87742334e+06, -7.98179221e+06,\n",
       "       -1.46242406e+06,  3.72654016e+08,  7.72713229e+09,  1.25927911e+11,\n",
       "        1.70335036e+12,  1.95018103e+13,  1.91317256e+14,  1.62340275e+15,\n",
       "        1.20060835e+16,  7.78768293e+16,  4.45373374e+17,  2.25562981e+18,\n",
       "        1.01547065e+19,  4.07673988e+19,  1.46351219e+20,  4.70913987e+20,\n",
       "        1.36091168e+21,  3.53849912e+21,  8.29008232e+21,  1.75228080e+22,\n",
       "        3.34520742e+22,  5.77310375e+22,  9.01331959e+22,  1.27381483e+23,\n",
       "        1.63030891e+23,  1.89022489e+23,  1.98572427e+23,  1.89022489e+23,\n",
       "        1.63030891e+23,  1.27381483e+23,  9.01331959e+22,  5.77310375e+22,\n",
       "        3.34520742e+22,  1.75228080e+22,  8.29008232e+21,  3.53849912e+21,\n",
       "        1.36091168e+21,  4.70913987e+20,  1.46351219e+20,  4.07673988e+19,\n",
       "        1.01547065e+19,  2.25562981e+18,  4.45373373e+17,  7.78768293e+16,\n",
       "        1.20060835e+16,  1.62340274e+15,  1.91317276e+14,  1.95018706e+13,\n",
       "        1.70338936e+12,  1.25966790e+11,  7.71324491e+09,  3.30174925e+08,\n",
       "       -4.11082157e+07, -7.52722355e+07, -6.85086986e+07, -5.89198673e+07,\n",
       "       -5.90773003e+07])"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = _denominator(diffc)\n",
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 1.49002560e+02, 1.06249299e+04, 4.83119899e+05,\n",
       "       1.57489104e+07, 3.92335692e+08, 7.77535874e+09, 1.26004530e+11,\n",
       "       1.70341958e+12, 1.95018803e+13, 1.91317294e+14, 1.62340277e+15,\n",
       "       1.20060835e+16, 7.78768293e+16, 4.45373373e+17, 2.25562981e+18,\n",
       "       1.01547065e+19, 4.07673988e+19, 1.46351219e+20, 4.70913987e+20,\n",
       "       1.36091168e+21, 3.53849912e+21, 8.29008232e+21, 1.75228080e+22,\n",
       "       3.34520742e+22, 5.77310375e+22, 9.01331959e+22, 1.27381483e+23,\n",
       "       1.63030891e+23, 1.89022489e+23, 1.98572427e+23, 1.89022489e+23,\n",
       "       1.63030891e+23, 1.27381483e+23, 9.01331959e+22, 5.77310375e+22,\n",
       "       3.34520742e+22, 1.75228080e+22, 8.29008232e+21, 3.53849912e+21,\n",
       "       1.36091168e+21, 4.70913987e+20, 1.46351219e+20, 4.07673988e+19,\n",
       "       1.01547065e+19, 2.25562981e+18, 4.45373373e+17, 7.78768293e+16,\n",
       "       1.20060835e+16, 1.62340277e+15, 1.91317294e+14, 1.95018803e+13,\n",
       "       1.70341958e+12, 1.26004530e+11, 7.77535874e+09, 3.92335692e+08,\n",
       "       1.57489104e+07, 4.83119899e+05, 1.06249299e+04, 1.49002560e+02,\n",
       "       1.00000000e+00])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vg = np.c_[np.ones_like(diffc), np.exp(-diffc)]\n",
    "g = 1\n",
    "\n",
    "for ndx in range(diffc.size):\n",
    "    g = np.convolve(g, vg[ndx])\n",
    "    \n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.50074565e+07, -3.06442698e+07, -7.88804827e+06, -8.46491211e+06,\n",
       "       -1.72113344e+07, -1.96816757e+07, -4.82264557e+07, -7.66188362e+07,\n",
       "       -6.92177030e+07, -7.00015650e+07, -3.77331717e+07, -2.02409685e+07,\n",
       "        1.05324200e+07,  4.15821600e+07,  5.98108160e+07,  6.47541760e+07,\n",
       "        6.26995200e+07,  4.92912640e+07,  3.07200000e+07,  1.76291840e+07,\n",
       "        2.07093760e+07,  3.25058560e+07,  7.65460480e+07,  9.43718400e+07,\n",
       "        1.42606336e+08,  1.76160768e+08,  1.84549376e+08,  1.34217728e+08,\n",
       "        1.67772160e+08,  1.34217728e+08, -3.35544320e+07, -6.71088640e+07,\n",
       "       -1.00663296e+08, -1.67772160e+08, -2.01326592e+08, -1.42606336e+08,\n",
       "       -5.87202560e+07,  4.19430400e+06,  5.97688320e+07,  1.38412032e+08,\n",
       "        1.59383552e+08,  1.65609472e+08,  1.51207936e+08,  1.10592000e+08,\n",
       "        6.05675520e+07,  1.26272000e+07, -1.56457600e+07, -2.72343360e+07,\n",
       "       -2.81049660e+07, -2.36520892e+07, -1.78709456e+07, -9.73387961e+06,\n",
       "       -3.02175351e+07, -3.77390551e+07, -6.21138344e+07, -6.21607668e+07,\n",
       "       -5.68571260e+07, -7.57553554e+07, -6.85193236e+07, -5.89200163e+07,\n",
       "       -5.90773013e+07])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8388608.               +0.j        ,\n",
       "        25165824.         -7077888.j        ,\n",
       "        16777216.         +9175040.j        ,\n",
       "       -23068672.        -20971520.j        ,\n",
       "         4194304.        +27262976.j        ,\n",
       "        -5242880.        -31195136.j        ,\n",
       "         -524288.         +3932160.j        ,\n",
       "         3145728.          +524288.j        ,\n",
       "         1703936.         +1114112.j        ,\n",
       "         1081344.         -1228800.j        ,\n",
       "          557056.          +679936.j        ,\n",
       "         -274432.           +86016.j        ,\n",
       "         -549888.          -309248.j        ,\n",
       "        -1100032.          +343808.j        ,\n",
       "         1925312.           -85952.j        ,\n",
       "        -1925264.          +360976.j        ,\n",
       "         4400582.          -773540.j        ,\n",
       "        -3300436.          +601642.j        ,\n",
       "         4400581.25        -618831.75j      ,\n",
       "        -2200290.62109375  +550072.65625j   ,\n",
       "         2200290.62304688  -962627.14794922j,\n",
       "               0.         +1272043.01635742j,\n",
       "        -1100145.31147385 -1237663.47541046j,\n",
       "        -1100145.3114754  +1306422.55737734j,\n",
       "        -1100145.31147541 -1375181.63934425j,\n",
       "         1100145.31147541 +1168904.39344262j,\n",
       "        -1100145.31147541  -687590.81967213j,\n",
       "         2200290.62295082  +825108.98360656j,\n",
       "        -2200290.62295082  -275036.32786885j,\n",
       "         1100145.31147541  +275036.32786885j,\n",
       "        -1100145.31147541       +0.j        ,\n",
       "        -1100145.31147541       +0.j        ,\n",
       "         1100145.31147541  -275036.32786885j,\n",
       "        -2200290.62295082  +275036.32786885j,\n",
       "         2200290.62295082  -825108.98360656j,\n",
       "        -1100145.31147541  +687590.81967213j,\n",
       "         1100145.31147541 -1168904.39344262j,\n",
       "        -1100145.31147541 +1375181.63934425j,\n",
       "        -1100145.3114754  -1306422.55737734j,\n",
       "        -1100145.31147385 +1237663.47541046j,\n",
       "               0.         -1272043.01635742j,\n",
       "         2200290.62304688  +962627.14794922j,\n",
       "        -2200290.62109375  -550072.65625j   ,\n",
       "         4400581.25        +618831.75j      ,\n",
       "        -3300436.          -601642.j        ,\n",
       "         4400582.          +773540.j        ,\n",
       "        -1925264.          -360976.j        ,\n",
       "         1925312.           +85952.j        ,\n",
       "        -1100032.          -343808.j        ,\n",
       "         -549888.          +309248.j        ,\n",
       "         -274432.           -86016.j        ,\n",
       "          557056.          -679936.j        ,\n",
       "         1081344.         +1228800.j        ,\n",
       "         1703936.         -1114112.j        ,\n",
       "         3145728.          -524288.j        ,\n",
       "         -524288.         -3932160.j        ,\n",
       "        -5242880.        +31195136.j        ,\n",
       "         4194304.        -27262976.j        ,\n",
       "       -23068672.        +20971520.j        ,\n",
       "        16777216.         -9175040.j        ,\n",
       "        25165824.         +7077888.j        ])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fft.ifft(g) - np.fft.ifft(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.59482185e+22-0.00000000e+00j, -2.45645048e+22+1.26622903e+21j,\n",
       "        2.08388164e+22-2.15408479e+21j, -1.58377521e+22+2.46666396e+21j,\n",
       "        1.07792280e+22-2.25254181e+21j, -6.56599976e+21+1.72918570e+21j,\n",
       "        3.57689313e+21-1.14187052e+21j, -1.74101777e+21+6.56339224e+20j,\n",
       "        7.56345780e+20-3.30542436e+20j, -2.92895765e+20+1.46398848e+20j,\n",
       "        1.00963621e+20-5.71415370e+19j, -3.09305612e+19+1.96738508e+19j,\n",
       "        8.40663789e+18-5.97645889e+18j, -2.02319640e+18+1.60130487e+18j,\n",
       "        4.30262339e+17-3.78147412e+17j, -8.06742774e+16+7.86231492e+16j,\n",
       "        1.33044577e+16-1.43741058e+16j, -1.92485231e+15+2.30732469e+15j,\n",
       "        2.43630444e+14-3.24659372e+14j, -2.68969164e+13+3.99749367e+13j,\n",
       "        2.58167635e+12-4.29941697e+12j, -2.14668054e+11+4.03183007e+11j,\n",
       "        1.54042347e+10-3.29057588e+10j, -9.43924677e+08+2.33272061e+09j,\n",
       "        5.17068296e+07-1.42950131e+08j,  2.20029062e+06+7.42598085e+06j,\n",
       "        1.10014531e+06-2.75036328e+05j,  2.20029062e+06-5.84452197e+05j,\n",
       "        2.20029062e+06+5.50072656e+05j,  3.30043593e+06-2.75036328e+05j,\n",
       "        4.40058125e+06+2.75036328e+05j,  4.40058125e+06-2.75036328e+05j,\n",
       "        3.30043593e+06+2.75036328e+05j,  2.20029062e+06-5.50072656e+05j,\n",
       "        2.20029062e+06+5.84452197e+05j,  1.10014531e+06+2.75036328e+05j,\n",
       "        2.20029062e+06-7.42598085e+06j,  5.17068296e+07+1.42950131e+08j,\n",
       "       -9.43924677e+08-2.33272061e+09j,  1.54042347e+10+3.29057588e+10j,\n",
       "       -2.14668054e+11-4.03183007e+11j,  2.58167635e+12+4.29941697e+12j,\n",
       "       -2.68969164e+13-3.99749367e+13j,  2.43630444e+14+3.24659372e+14j,\n",
       "       -1.92485231e+15-2.30732469e+15j,  1.33044577e+16+1.43741058e+16j,\n",
       "       -8.06742774e+16-7.86231492e+16j,  4.30262339e+17+3.78147412e+17j,\n",
       "       -2.02319640e+18-1.60130487e+18j,  8.40663789e+18+5.97645889e+18j,\n",
       "       -3.09305612e+19-1.96738508e+19j,  1.00963621e+20+5.71415370e+19j,\n",
       "       -2.92895765e+20-1.46398848e+20j,  7.56345780e+20+3.30542436e+20j,\n",
       "       -1.74101777e+21-6.56339224e+20j,  3.57689313e+21+1.14187052e+21j,\n",
       "       -6.56599976e+21-1.72918570e+21j,  1.07792280e+22+2.25254181e+21j,\n",
       "       -1.58377521e+22-2.46666396e+21j,  2.08388164e+22+2.15408479e+21j,\n",
       "       -2.45645048e+22-1.26622903e+21j])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.fft.ifft(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
